{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, StorageLevel\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import itertools\n",
    "from time import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following pararmeters are configurable and can be changed as per the business requirements\n",
    "filter_threshold = 1\n",
    "supportThreshold = 30\n",
    "input_file_path = \".\\preometheus-sample.csv\"\n",
    "output_file_path = \".\\kiana-match-results.txt\"\n",
    "mapid_map_path = \".\\macid-map-john.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(entry):\n",
    "    revisedEntries= entry[0].replace('\\'', '').split(',')\n",
    "    return (revisedEntries[1], str(revisedEntries[0]), str(revisedEntries[2]),str(revisedEntries[3]), str(revisedEntries[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertValuesToTuple(entrySet):\n",
    "    newEntrySet = []\n",
    "    for entry in entrySet:\n",
    "        newEntrySet += [(entry, 1)]\n",
    "    return newEntrySet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_k_candidates(k, current_candidate_set):\n",
    "\n",
    "    new_k_candidate_set = set()\n",
    "    current_candidate_list_flattened = frozenset(itertools.chain.from_iterable(current_candidate_set))\n",
    "    new_candidate = frozenset()\n",
    "    \n",
    "    for old_candidate in current_candidate_set:    \n",
    "        for single_item in current_candidate_list_flattened:\n",
    "            if single_item not in old_candidate:\n",
    "                new_candidate = frozenset(sorted(old_candidate.union(frozenset([single_item]))))               \n",
    "                if len(new_candidate) == k:\n",
    "                    k_minus_one_subsets = itertools.combinations(new_candidate, k-1)\n",
    "                    is_valid_candidate = True\n",
    "\n",
    "                    for subset in k_minus_one_subsets:\n",
    "                        subset_frozen = frozenset(subset)\n",
    "                        if not subset_frozen in current_candidate_set:\n",
    "                            is_valid_candidate = False\n",
    "                            break\n",
    "                            \n",
    "                    if is_valid_candidate:\n",
    "                        new_k_candidate_set.add(new_candidate)\n",
    "\n",
    "    new_k_candidate_set = frozenset(sorted(new_k_candidate_set))\n",
    "    return new_k_candidate_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_generate_frequent_k_candidates(original_baskets, current_candidate_set, partition_support_threshold):\n",
    "   \n",
    "    current_k_frequent_candidates = {}\n",
    "    current_k_frequents = set()\n",
    "    \n",
    "    for key, values in original_baskets.items():    \n",
    "        basket_value_set = frozenset([values])\n",
    "       \n",
    "        for candidate in current_candidate_set:\n",
    "            if candidate.issubset(values):\n",
    "                if candidate in current_k_frequent_candidates.keys():\n",
    "                    current_k_frequent_candidates[candidate] += 1\n",
    "                else:\n",
    "                    current_k_frequent_candidates.update({candidate:1})\n",
    "                     \n",
    "    for key, value in current_k_frequent_candidates.items():\n",
    "        if value >= partition_support_threshold:\n",
    "            current_k_frequents.add(key)\n",
    "   \n",
    "    current_k_frequents = frozenset(sorted(current_k_frequents))\n",
    "    return current_k_frequents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_implementation(baskets, support_threshold, total_baskets_count):\n",
    "    original_baskets = {}\n",
    "    for key, values in baskets:\n",
    "        original_baskets.update({key:frozenset(values)})\n",
    "\n",
    "    partition_support_threshold = math.ceil((float(len(original_baskets))/total_baskets_count) * support_threshold)\n",
    "    all_frequent_items_set ={}\n",
    "    \n",
    "    # calculate singleton matches\n",
    "    single_frequent_items_candidates = {}\n",
    "    single_frequent_items = set()\n",
    "    for key, values in original_baskets.items():\n",
    "        for val in values:\n",
    "            if val in single_frequent_items_candidates.keys():\n",
    "                single_frequent_items_candidates[val] += 1\n",
    "            else:\n",
    "                single_frequent_items_candidates.update({val:1})\n",
    "\n",
    "    for key, value in single_frequent_items_candidates.items():\n",
    "        if value >= partition_support_threshold:\n",
    "            single_frequent_items.add(frozenset([key]))\n",
    "\n",
    "    single_frequent_items = frozenset(sorted(single_frequent_items))\n",
    "    all_frequent_items_set.update({1:single_frequent_items})\n",
    "\n",
    "    current_candidate_set = single_frequent_items\n",
    "    current_frequent_items = set()\n",
    "   \n",
    "    # calculate matches of length >1 to identify devices belonging to same person or a group of people walking together\n",
    "    k=2\n",
    "    \n",
    "    while len(current_candidate_set) != 0 :\n",
    "        current_candidate_set = generate_k_candidates(k, current_candidate_set)\n",
    "        current_frequent_items = check_and_generate_frequent_k_candidates(original_baskets, current_candidate_set, partition_support_threshold)\n",
    "\n",
    "        if len(current_frequent_items) != 0:\n",
    "            all_frequent_items_set.update({k:current_frequent_items})\n",
    "\n",
    "        k += 1\n",
    "        current_candidate_set = current_frequent_items\n",
    "\n",
    "    # writing all teh potential candidate pairs =, can be used to fine tune the results\n",
    "    with open(output_file_path, \"w+\") as op:\n",
    "        op.write(\"Candidates: \" + '\\n\\n')\n",
    "        \n",
    "        for key, itemset in all_frequent_items_set.items():\n",
    "            values = sorted([tuple(sorted(i)) for i in itemset])\n",
    "            length = len(values)\n",
    "            for index, tuple_to_write in enumerate(values):\n",
    "                tuple_to_write_string = ''\n",
    "                if key == 1:\n",
    "                    tuple_to_write_string = '(\\'' + tuple_to_write[0] + '\\')'\n",
    "                else:\n",
    "                    tuple_to_write_string = str(tuple_to_write)\n",
    "\n",
    "                if index != length - 1:\n",
    "                    tuple_to_write_string += ','\n",
    "\n",
    "                op.write(tuple_to_write_string)\n",
    "\n",
    "            op.write(\"\\n\\n\")\n",
    "\n",
    "    return list(all_frequent_items_set.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frequent_candidates(basket, candidate_list):\n",
    "    frequent_candidate_counts = []\n",
    "    for candidate in candidate_list:\n",
    "        if candidate.issubset(basket):\n",
    "            frequent_candidate_counts.append((candidate, 1))\n",
    "\n",
    "    return frequent_candidate_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def son_implementation(basketsRdd, support_threshold, total_baskets_count):\n",
    "\n",
    "    map_task_1 = basketsRdd\\\n",
    "        .mapPartitions(lambda entrysets: apriori_implementation(entrysets, support_threshold, total_baskets_count))\\\n",
    "        .map(lambda entry: (entry, 1))\n",
    "    \n",
    "    reduce_task_1 = map_task_1.reduceByKey(lambda x, y: x+y)\\\n",
    "        .map(lambda entry: entry[0])\n",
    "\n",
    "    task1_candidates = reduce_task_1.collect()\n",
    "\n",
    "    task1_candidates_broadcasted = sc.broadcast(task1_candidates).value\n",
    "    task1_candidates_broadcasted = frozenset(itertools.chain.from_iterable(task1_candidates_broadcasted))\n",
    "\n",
    "    map_task_2 = basketsRdd.flatMap(lambda entry: count_frequent_candidates(entry[1], task1_candidates_broadcasted))\n",
    "\n",
    "    reduce_task_2 = map_task_2.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "    frequent_itemsets = reduce_task_2.filter(lambda entry: entry[1] >= supportThreshold)\\\n",
    "        .map(lambda entry: (len(entry[0]), frozenset([entry[0]])))\\\n",
    "        .reduceByKey(lambda set1, set2: set1.union(set2)).sortByKey().collect()\\\n",
    "\n",
    "    with open(output_file_path, \"a+\") as op:\n",
    "        op.write(\"Frequent Itemsets: \" + '\\n\\n')\n",
    "        for itemset in frequent_itemsets:\n",
    "            values = sorted([tuple(sorted(i)) for i in itemset[1]])\n",
    "            length = len(values)\n",
    "            for index, tuple_to_write in enumerate(values):\n",
    "                tuple_to_write_string = ''\n",
    "                if itemset[0] == 1:\n",
    "                    tuple_to_write_string = '(\\'' + tuple_to_write[0] + '\\')'\n",
    "                else:\n",
    "                    tuple_to_write_string = str(tuple_to_write)\n",
    "\n",
    "                if index != length - 1:\n",
    "                    tuple_to_write_string += ','\n",
    "\n",
    "                op.write(tuple_to_write_string)\n",
    "\n",
    "            op.write(\"\\n\\n\")\n",
    "\n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    result = {}\n",
    "    SparkContext.setSystemProperty('spark.executor.memory', '8g')\n",
    "    SparkContext.setSystemProperty('spark.driver.memory', '4g')\n",
    "    sc = SparkContext('local[*]', 'kiana-son-local-task')\n",
    "\n",
    "    global_macid_dict = {}\n",
    "    global_macid_revers_dict = {}\n",
    "\n",
    "    with open(macid_map_path, \"r\") as fp:\n",
    "    global_macid_dict = json.load(fp)\n",
    "\n",
    "    for key, val in global_macid_dict.items():\n",
    "    global_macid_revers_dict[val] = key\n",
    "\n",
    "    start = time()\n",
    "    user_businessRdd = sc.textFile(input_file_path, 5).map(lambda entry: entry.split('\\n')).map(lambda entry: process(entry))\n",
    "    headers = user_businessRdd.take(1)\n",
    "    finalRdd = user_businessRdd.filter(lambda entry: entry[0] != headers[0][0])\n",
    "\n",
    "\n",
    "    finalRdd = finalRdd.map(lambda entry: ((entry[1]+entry[2]+entry[3]+entry[4]), entry[0]))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(lambda entry: frozenset(entry)).distinct()\n",
    "    .filter(lambda entry: len(entry[1]) > filter_threshold)\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "\n",
    "    baskets = finalRdd.collect()\n",
    "\n",
    "    total_baskets_count = finalRdd.count()\n",
    "\n",
    "    with open(\"./baskests_user_business.json\", \"w+\") as f:\n",
    "    for item in baskets:\n",
    "        f.write(str(item))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    results = son_implementation(finalRdd, supportThreshold, total_baskets_count)\n",
    "\n",
    "    # writing the final results to the output file configured\n",
    "    with open(output_file_path, \"a+\") as op:\n",
    "        for itemset in results:\n",
    "            values = sorted([tuple(sorted(i)) for i in itemset[1]])\n",
    "            length = len(values)\n",
    "            \n",
    "            for index, tuple_to_write in enumerate(values):\n",
    "                tuple_to_write_string = ''\n",
    "                if itemset[0] == 1:\n",
    "                    tuple_to_write_string = global_macid_revers_dict[int(tuple_to_write[0])]\n",
    "                else:\n",
    "                    for x in tuple_to_write[:-1]:\n",
    "                        tuple_to_write_string += global_macid_revers_dict[int(x)] + \",\"\n",
    "                    tuple_to_write_string += global_macid_revers_dict[int(tuple_to_write[-1])]\n",
    "\n",
    "                op.write(tuple_to_write_string)\n",
    "                op.write(\"\\n\")\n",
    "                \n",
    "    end = time()\n",
    "    print(\"Duration to implement matching: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
